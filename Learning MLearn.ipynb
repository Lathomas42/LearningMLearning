{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Machine Learning\n",
    "Code from my experience learning the basics of machine learning from the book \"Introduction to Machine Learning\" by Alex Smola and SVN Vishwanathan\n",
    "\n",
    "## Part 1: Basic Algorithms\n",
    "----\n",
    "### 1. Naive Bayes\n",
    "Naive bayes uses bayes rule from statistics to classify a variable. An example of this used in the text is spam vs ham email. It will do this by using the various word counts of different words in each email and comparing them to the word's frequenct in spam emails vs the word's frequency in ham emails.\n",
    "\n",
    "### Mathematics\n",
    "#### Baye's Rule\n",
    "The Naive bayes classifiers are based on \"Bayes Rule\" Which simply provides a method of calculating posterior probability from independent probabilities and the likelyhood probability. In english, it is a method of calculating the probability of an event, based on the prior knowledge of other conditions which might be related to the event. The basic equation is as follows:\n",
    "$$ P( c \\mid x ) = \\frac{P ( x \\mid c ) P( c )}{ P( x )} $$\n",
    "\n",
    "Where : \n",
    "- $P(c \\mid x)$ is the probabilty of class $c$ given predictor $x$.\n",
    "- $P(x \\mid c)$ is the probability of predictor $x$ given class $c$.\n",
    "- $P(x)$ is the probability of predictor $x$\n",
    "- $P(c)$ is the probability of class $c$\n",
    "\n",
    "#### Baye's Rule Example\n",
    "A simple example of Bayes rule in action would be this:\n",
    "A Doctor gives you a blood test for the menacingly named *Scary Flu*. Let's Say the *Scary Flu* affects 1 in 1000 People in your country. As the test is being run, you read that the test is 99% accurate which means it's false positive rate is 1%. You get back a positive result. What is the probability you have the *Scary Flu*?\n",
    "\n",
    "For this example:\n",
    "- The classifier $c$ will be if you have Scary flu or not\n",
    "- The predictor $x$ will be the fact you got a positive result from the test\n",
    "- What we want to calculate: The probability $P( c \\mid x )$, the probability that you have Scary flu given the fact you got a positive result from the test\n",
    "\n",
    "In order to calculate this value using Bayes rule we must first calculate:\n",
    "- $P(c)$ the probability of having the swine flu\n",
    "$$P(c) = 1 / 1000 = .001 $$\n",
    "- $P(x \\mid c)$ the probability of getting a positive result on the test given you have Scary flu.\n",
    "$$P(x \\mid c) = 1.0 $$\n",
    "- $P(x)$ the probability of getting a positive result on the test\n",
    "$$P(x) = P(x \\mid c)*P(c) + P(x \\mid !c)*P(!c)$$\n",
    "$$P(x) = 1.0*.001 + .01*.999 = 0.01099 $$ \n",
    "\n",
    "Now using Bayes Rule:\n",
    "$$P( c \\mid x ) = \\frac{P(x \\mid c) * P(c)}{P(x)}$$\n",
    "$$P(c \\mid x) = \\frac{ 1.0 * .001}{0.01099} \\approx 0.09$$\n",
    "This means, given a positive on the test, you have around 9% chance of actually having the Scary Flu. Not so bad!\n",
    "\n",
    "#### Naive Bayes Transition and Example\n",
    "Naive Baye's classifiers use Bayes rule to predict a certain class of an object based on some predictor. This is *naive* because it assumes independence, even when that clearly is not the case. To give an example with context I will show a basic spam email filter. First, the mathematics behind it. \n",
    "\n",
    "The problem will be to read an email, and try to predict if that email is *spam* or *ham*. Ham being a word for an email that a person cares about. To put this into the context of Bayes rule, for any given email $x$ we want to know:\n",
    "$$P(\\text{ham} \\mid x ) \\text{ and } P( \\text{spam} \\mid x )$$\n",
    "We know from N total training emails with $n_{\\text{ham}}$ ham emails and $n_{\\text{spam}}$ spam emails:\n",
    "$$P(\\text{ham})\\approx \\frac{n_{\\text{ham}}}{N}$$\n",
    "$$P(\\text{spam})\\approx \\frac{n_{\\text{spam}}}{N}$$\n",
    "However, the other necessary quantities, specifically $P(x)$, $P(x \\mid spam)$, and $P( x \\mid ham )$ are unknown. We can simplify our classifier, as our problem is binary, by reducing our problem down to a Likelyhood ratio:\n",
    "$$ L(x) = \\frac{P(\\text{spam}\\mid x)}{P(\\text{ham}\\mid x)} $$\n",
    "And using bayes rule simplify that to:\n",
    "$$ L(x) = \\frac{P(x\\mid\\text{spam})P(\\text{spam})}{P(x\\mid\\text{ham})P(\\text{ham})}$$\n",
    "This effectively removes our dependency on $P(x)$ however we still need to calculate the other two quantities. \n",
    "\n",
    "The calculation of the quantities $P(\\text{ham} \\mid x)$ and $P(\\text{spam} \\mid x)$ are where we make our key approximation. There are many ways to do this step. In order to do this step we must decide on what metrics we will use. For this example, we will use only the probability of finding each word in a spam or ham document. The details on how we choose to do this follows.\n",
    "###  1.i. Training\n",
    "To show an example of this algorithm I will use [data from the University of California Irvine's machine learning databases](https://archive.ics.uci.edu/ml/index.php) which are a great place to go to get some nice datasets to learn on. The goal of the training algorithm is to get an idea of the dataset and teach an algorithm how to distinguish spam from ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "# The data I am using is from https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/\n",
    "labelsFile = 'SPAMTrain.label'\n",
    "\n",
    "indir = 'ExtractedData/'\n",
    "# now I build a frequency distrubtion of all the words in each type\n",
    "# of email\n",
    "fdistSpam = nltk.FreqDist()\n",
    "fdistHam = nltk.FreqDist()\n",
    "\n",
    "# we will train on the first 90% of our data\n",
    "nTrain = 1\n",
    "nTest = 1\n",
    "with open(labelsFile, 'r') as flabel:\n",
    "    nTotalLines = len(flabel.readlines())\n",
    "    nTrain = int(nTotalLines * .9)\n",
    "    nTest = int(nTotalLines * .1)\n",
    "    \n",
    "nSpam = 0\n",
    "nHam = 0\n",
    "with open(labelsFile, 'r') as flabel:\n",
    "    # only go over the first nTrain lines\n",
    "    # we could randomize which nTrain files to train on, for \n",
    "    # simplicity's sake I will just go over the first ones\n",
    "    for _ in range(nTrain):\n",
    "        line = flabel.readline()\n",
    "        label = int(line.split()[0])\n",
    "        fname = line.split()[1]\n",
    "        with open(indir + fname, 'r', encoding=\"latin-1\") as myfile:\n",
    "            totEmail = myfile.read()\n",
    "            totEmail = re.sub('<[^<]+?>', '', totEmail)\n",
    "            for sent in nltk.sent_tokenize(totEmail):\n",
    "                for w in nltk.word_tokenize(sent):\n",
    "                    if( len(w) > 2 ):\n",
    "                        if( label == 1 ): #spam\n",
    "                            nSpam += 1\n",
    "                            fdistSpam[w] += 1\n",
    "                        else: #ham\n",
    "                            nHam += 1\n",
    "                            fdistHam[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we can define our classify method as follows\n",
    "def Classify( fEmail, c = .5 ):\n",
    "    with open(fEmail, 'r', encoding=\"latin-1\") as myfile:\n",
    "        # first make a freqDist for our new file\n",
    "        fdistEmail = nltk.FreqDist()\n",
    "        totEmail = myfile.read()\n",
    "        totEmail = re.sub('<[^<]+?>', '', totEmail)\n",
    "        for sent in nltk.sent_tokenize(totEmail):\n",
    "            for w in nltk.word_tokenize(sent):\n",
    "                if( len(w) > 2 ):\n",
    "                    fdistEmail[w] += 1\n",
    "                    \n",
    "        # initialize this to offset the rejection threshold\n",
    "        t = -1*((np.log(c) + np.log(nHam) - np.log(nSpam)))\n",
    "        offsetSpam = 1.0 / (fdistSpam.N() + len(fdistSpam))\n",
    "        offsetHam = 1.0 / (fdistHam.N() + len(fdistHam))\n",
    "        for w in fdistEmail.keys():\n",
    "            t += fdistEmail[w] * ( np.log(fdistSpam.freq(w) + offsetSpam) - np.log(fdistHam.freq(w) + offsetHam))\n",
    "        if( t > 0 ):\n",
    "            return 1 #spam\n",
    "        else:\n",
    "            return 0 #ham\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.99074074074075 % Correct\n"
     ]
    }
   ],
   "source": [
    "# now we test ourselves\n",
    "nCorrect = 0\n",
    "with open(labelsFile, 'r') as flabel:\n",
    "    # skip the training data\n",
    "    for _ in range(nTrain):\n",
    "        flabel.readline()\n",
    "    # now read the last nTest lines and classify the documents\n",
    "    for _ in range(nTest):\n",
    "        line = flabel.readline()\n",
    "        label = int(line.split()[0])\n",
    "        fname = line.split()[1]\n",
    "        spam = Classify(indir + fname)\n",
    "        # if we were correct in our classification, increment our \n",
    "        # correct count\n",
    "        if( spam == label ):\n",
    "            nCorrect += 1\n",
    "    print(( 100.0 * nCorrect )/nTest ,\"% Correct\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now our algorithm is ready to use and classify actual data simply using the classify function. The bulk of the work was done in training. classifying is lower weight as the frequency distributions have been already calculated.\n",
    "\n",
    "Some possible ways to optimize this specific problem which would serve as good beginner exercises would be to try to pair down the dictionary to only the most polarizing words. i.e. the words that show up in spam *significantly* more than ham or in ham more than spam. This would cut down algorithm complexity on the Classifying side and would also cut down the size of the dictionaries from the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 Nearest Neighbor Estimators\n",
    "Another simple algorithm used in classifiers in machine learning is the Nearest Neighbors algorithm. It assigns the label to an observation based on the nearest neighbor(s) to that obersvation. In order to define \"nearest\" we need to have a distance metric. This distance is very problem specific and it allows the algorithm to be extremely flexible. For instance, in a problem involving categorizing documents we could use a distance metric of how many characters you would have to change to get from one to the other. \n",
    "\n",
    "In it's simplest form, this algorithm looks at only the nearest neighbor of an observation. This is prone to some serious noise, as one mislabeled element can cause a pocket around it of mislabled observations. To remedy this issue with the algorithm, it helps to take a few of the nearest neighbors and take the average label among them. This small adaption to the Nearest Neighbor algorithm ears the new name k-Nearest Neighbor. This algorithm can yeild excellent performance if the distance metric is good. This is a very popular algorithm because of it's ease of use. \n",
    "\n",
    "This algorithm requires very little mathematics to back it up and is most easily shown in an example. For this we will skip the mathematics section and go straight to the example.\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
